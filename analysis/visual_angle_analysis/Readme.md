# Visual Angle Detection Manual

The Pats system has a component that is used to calculate yaw and correct it, after aggressive maneuvres have occurred when hunting a moth. Up until now it was not clear how well this system worked. In order to quantify this, the following software and experiment where made. Using a GoPro placed on the ceiling of the test fly room, a ground truth can be made with which the current and future yaw approaches can be compared with. The following text will explain how to setup the experiment and use the software.

##Components
 - Software:
   - ```video_angle_detection.py```
   - ```angle_comparison.py```
   - Pats system software
- GoPro camera taped to ceiling above landing and yaw correction region. This may require changes to be made to the pats system software to facilitate this. The GoPro can be connected to with an app, allowing video to be recorded remotely.
- light with switch that can be seen from both pats system and GoPro
- Tape
- Either ROS with rqt_rosbag or the pats program in logging mode
- preliminary directory setup
  |-> `code`
  |-> `pats_yaw_data`
  contains the yaw data generated by the pats system. must be of the name format `{0}_pats_log.csv`, where `{0}` corresponds to the corresponding gopro video name
  |-> `videos`
  contains the gopro videos shot from the ceiling
  |-> `visual_angle_data`
  

## Data Gathering Method
In order to perform a comparison between the yaw component implemented in the pats system and the actual yaw that the drone has, needs to be gathered. The following steps will make that happen.

0. start recording remotely on the GoPro 
1. Start the pats system to fly the drone through a trajectory in which aggressive maneuvres are performed
2. Blink the light 
3. Once the drone has landed, stop the video record
4. Make sure that the pats system has closed correctly
5. Go to the build folder of the pats system and rename the logging folder in order to save the pats system data

## Visual Angle Measurement Method

Now that video and data has been generated through the data gathering method, the first thing to do is to determine when the blink has occurred. This is for both syncronization and mitigation of overexposure caused by the blink. This is done with ```video_angle_detection.py``` script. There are different control variables that need to be set in order for the script to work.

0. Set `filename` and `extension` to those of the video you want to analyse
1. Set `find_blink_frame` to `True`  
2. Set `blink_frame` to an initial frame approximation of where the light blink occurs
3. run the `video_angle_detection.py` script. While the script is running, there are different video controls that can be used. This includes pause/play (`p`), next frame (`n`) and previous frame (`b`).
4. if `blink_frame` is at the right location, the whole video should play and you should not see the blink anymore. Otherwise it will still show, or worse it will crash the program. Watch the frame number in the generated video to determine the the correct blink frame. Use the video controls accordingly to find the frame. The blink frame that is indicated will be saved in the settings csv file. This file can be used in the future, so that you do not have to indicate the blink frame each time.
5. Once the blink frame has been found, set `find_blink_frame` to `False`, and `tune_reference_detection` to `True`. This will generate a window in which you need to indicate what the reference tape is. Press `enter` once the tape is inside the indicator box. This tape is expected to be positioned as the absolute zero. 
6. A second window will indicate 6 sliders and the image of the tape you just made. Adjust the slide such that only the tape is seen in the picture. Once you are satisfied, press `d` . The tape will now be detected and the angle within the frame will be calculated.
7. The video will now play, tracking the drone orientation, based on the position of its red components. 
8. If you want to run the script again , but do not want to do reference tuning again, set both `find_blink_frame` and `tune_reference_detection` to `false`

## Angle Comparison Method

Once visual data has ben gathered, the data generated by the pats system and the ground truth detected through the visual angle detection method can be compared. This is done with the `angle_comparison.py` script.

0. The pats log file that you want to compare must be of the name format `{0}_pats_log.csv` in the `/pats_yaw_data/` folder. The visual data log file that is made in the previous method will be of the format `{0}_visual_angle_data.csv` in the `/visual_angle_data/` folder. For both cases the `{0}` represents the name of the video file that was used to determine the visual angle. 
1. Set `filename` to the name that was indicated by the `{0}` mentioned before. 
2. First the light blink needs to be detected in the video of the pats system. This is done through the use of either RQT or by converting the video from .bag file to a video format. Playing and scrolling through the video will help determine this time.
3. Indicate this flash time in the script
4. Run the script to generate the plots where the yaw calculation of ground truth and pats system can be found 